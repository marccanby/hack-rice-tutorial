{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Tutorial $-$ Hack Rice 2017 \n",
    "### Marc E. Canby\n",
    "\n",
    "This tutorial will teach you how to:\n",
    "* Train a binary classifier in Python using a classical machine learning algorithm\n",
    "* Process text using the Python library sklearn\n",
    "* Select an appropriate error metric for your problem\n",
    "\n",
    "This tutorial will not cover:\n",
    "* Neural networks\n",
    "* Unsupervised learning\n",
    "* Big data computing algorithms such as Spark or MapReduce\n",
    "* Big data computing resources such as AWS or Google Cloud\n",
    "\n",
    "If you plan to run the code locally, you will need to have Python 3 installed, as well as the following libraries:\n",
    "* numpy\n",
    "* pandas\n",
    "* sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in the data\n",
    "Before training a model, we will need some data. For our purposes, can use a dataset built into sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Document 1:', u\"From: jodfishe@silver.ucs.indiana.edu (joseph dale fisher)\\nSubject: Re: prayers and advice requested on family problem\\nOrganization: Indiana University\\nLines: 34\\n\\nJulie, it is a really trying situation that you have described.  My\\nbrother was living with someone like that and things were almost as bad\\n(although he left after a considerably shorter amount of time due to\\nother problems with the relationship).  Anyway, the best thing to do\\nwould be to get everyone in the same room together (optimally in a room\\nwith nothing breakable), lock the door behind you, throw the key out\\nunderneath the door (just as far as the longest hand can reach.  You\\nwould like to get out after the conclusion, I would imagine), and hash\\nthings out.  More than likely, there will be screaming, crying, and\\npossibly hitting (unless of course someone decided to bring some rope to\\ntie people down).  Some of the best strategies in keeping things calmer\\nwould include:\\n   have each individual own their own statements (ie, I feel that this\\nrelationship is hurting everyone involved because.... or I really don't\\nunderstand where you're coming from.)\\n   reinforce statements by paraphrasing, etc. (ie, So you think that we\\ndid this because of...?  Well, let me just say that the reason for this\\nwas ....)\\n   don't accuse each other (It was your fault that ... happened!)\\n   find a common ground about SOMETHING (Lampshades really are\\ndecorational and functional at the same time.)\\n   Guaranteed, in a situation like this, there is going to be some\\ngunnysacking (re-hashing topics which were assumed resolved, but were\\ntruly not and someone feels someone else is to blame).  However, this\\nshould be kept to a minimum and simply ask for forgiveness or apologize\\nabout each situation WITHOUT holding a smoldering grudge.  \\n\\nThe relationship really can work.  It's just a matter of keeping things\\nsmooth and even.  It's sort of like making a peace treaty between\\nwarring factions:  you can't give one side everything; there must be a\\ncompromise.  Breaks can be taken, but communication between everyone\\ninvolved must continue if the relationships here are to survive.\\n\\nJoe Fisher\\n\")\n",
      "('Document 1 Class:', 15)\n",
      "('Document 1 Class Name:', 'alt.atheism')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "random_seed = 1147\n",
    "twenty_train = fetch_20newsgroups(shuffle=True, random_state=random_seed)\n",
    "\n",
    "document_1 = twenty_train.data[0]\n",
    "document_1_class = twenty_train.target[0]\n",
    "document_1_class_name = twenty_train.target_names[0]\n",
    "print(\"Document 1:\", document_1)\n",
    "print(\"Document 1 Class:\", document_1_class)\n",
    "print(\"Document 1 Class Name:\", document_1_class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also helpful to get a sense of the dataset as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('All classes:', ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'])\n",
      "('Number of documents:', 11314)\n"
     ]
    }
   ],
   "source": [
    "print(\"All classes:\", twenty_train.target_names)\n",
    "print (\"Number of documents:\", len(twenty_train.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to predict the class of a document from only the text within it. We will build a classifier to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split the data into a train and a test set\n",
    "\n",
    "In machine learning, it is important to partition data into a training and a test set. The training set will be used to train the model (i.e. we will feed examples in the training set into the model). The test set will be used to score the model.\n",
    "\n",
    "It is typical to use 80% of the data we have as the training set and 20% as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of training examples:', 9051)\n",
      "('Number of testing examples:', 2263)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idxes, test_idxes = train_test_split(range(len(twenty_train.data)), test_size = 0.20, random_state = random_seed)\n",
    "train_data = [twenty_train.data[i] for i in range(len(twenty_train.data)) if i in train_idxes]\n",
    "test_data = [twenty_train.data[i] for i in range(len(twenty_train.data)) if i in test_idxes]\n",
    "train_labels = [twenty_train.target[i] for i in range(len(twenty_train.target)) if i in train_idxes]\n",
    "test_labels = [twenty_train.target[i] for i in range(len(twenty_train.target)) if i in test_idxes]\n",
    "\n",
    "print (\"Number of training examples:\", len(train_data))\n",
    "print(\"Number of testing examples:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Transform the data into a workable format\n",
    "\n",
    "In order to build a model from the data, we need to get the data into a numerical format. We will construct a matrix in which each row represents one of our documents, and where each column represents a \"feature\" of that document.\n",
    "\n",
    "In our case, we will have 11,314 rows (one for each document). We will have a column for each unique word in the entire document corpus. An entry at row $i$ and column $j$ of the matrix will represent the number of times word $j$ occurs in document $i$.\n",
    "\n",
    "Fortunately, we don't have to do this counting ourselves. There is a handy Python library that can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training matrix: (9051, 115114)\n",
      "Shape of test matrix: (2263, 115114)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_counts = cv.fit_transform(train_data)\n",
    "print (\"Shape of training matrix: \" +str(X_train_counts.shape))\n",
    "\n",
    "X_test_counts = cv.transform(test_data)\n",
    "print(\"Shape of test matrix: \" + str(X_test_counts.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training matrix now has a row for each document, and each column represents the number of times a word appears in a given document. But what if some documents have only a couple of words, and some have thousands? The counts for longer documents will naturally be higher, causing our data to be dependent on document length. Can we normalize the data?\n",
    "\n",
    "Yes we can. We can calculate the term frequency of word $j$ in document $i$. This is given by\n",
    "\n",
    "$$TF(i,j)=\\frac{\\textrm{Number of times word } j \\textrm{ appears in document } i}{\\textrm{Number of words in document }i}$$\n",
    "\n",
    "But we can do even better. Will all words be equally likely in determining what class a document belongs to? No. Common words like \"the\", \"a\", and \"where\" occur very frequently and are not likely to be very predictive. We can penalize such words using what's called the inverse document frequency transformation on document $i$:\n",
    "\n",
    "$$IDF(i) = \\log \\frac{\\textrm{Number of documents}}{\\textrm{Number of documents containing word } i}$$\n",
    "\n",
    "Multiplying $TF(i,j)$ and $IDF(i,j$ gives the TFIDF value of document $i$ and word $j$, and we can use these values in our matrix instead of the counts:\n",
    "\n",
    "$$TFIDF(i,j)=TF(i,j)\\times IDF(i)$$\n",
    "\n",
    "We don't have to perform any of this math ourselves. There is a built-in method to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training matrix: (9051, 115114)\n",
      "Shape of test matrix: (2263, 115114)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (\"Shape of training matrix: \" + str(X_train.shape))\n",
    "\n",
    "X_test = tfidf_transformer.transform(X_test_counts)\n",
    "print (\"Shape of test matrix: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training a binary classifier\n",
    "To simplify things, we will start with training a binary classifier. A binary classifier is a function that will classify a given document as \"yes\" or \"no\".\n",
    "\n",
    "For the moment, we will be interested in determining if a document is about computer graphics. All documents about politics will be given the label $1$ and all other documents will be given the label $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 1's in training data: 486 / 9051 = 0.0536957242294\n",
      "Proportion of 1's in testing data: 98 / 2263 = 0.0433053468847\n"
     ]
    }
   ],
   "source": [
    "y_train = [1 if train_labels[i] == twenty_train.target_names.index('comp.graphics') else 0 for i in range(len(train_labels))]\n",
    "y_test = [1 if test_labels[i] == twenty_train.target_names.index('comp.graphics') else 0 for i in range(len(test_labels))]\n",
    "\n",
    "print(\"Proportion of 1's in training data: \" + str(sum(y_train)) + \" / \" + str(len(y_train)) + \" = \" + str(1.*sum(y_train)/len(y_train)))\n",
    "print(\"Proportion of 1's in testing data: \" + str(sum(y_test)) + \" / \" + str(len(y_test)) + \" = \" + str(1.*sum(y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train a binary classifier using logistic regression. Logistic regression is a modification of linear regression that produces an output between $0$ and $1$. This output is interpreted as the probability that a given document has label $1$.\n",
    "\n",
    "Typically, if the output is greater than $0.5$, label $1$ is predicted. Otherwise, label $0$ is predicted.\n",
    "\n",
    "Sklearn offers a convenient way to perform logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of predictions that are 1: 17 / 2263 = 0.00751215201061\n",
      "Accuracy of prediction: 0.962439239947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "print(\"Proportion of predictions that are 1: \" + str(sum(y_predicted)) + \" / \" + str(len(y_predicted)) + \" = \" + str(1.*sum(y_predicted)/len(y_predicted)))\n",
    "print(\"Accuracy of prediction: \" + str(1.*sum(y_predicted == y_test) / len(y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 96% accuracy!! But is accuracy really a good metric here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating a binary classifier\n",
    "In order to say whether our classifier is good or not, we need to find a numerical way to evaluate it. The first thing that comes to mind is accuracy, which is defined as\n",
    "\n",
    "$$ Accuracy = \\frac{\\textrm{Number of correctly classified documents}}{\\textrm{Number of classified documents}}$$\n",
    "\n",
    "But we said that less than 5% of our data has label \"1\". Therefore, if we always predicted label $0$, we would have about 95% accuracy! Clearly, accuracy is not a good error metric if we wish to predict a rare event. So, we have to do something more clever.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>&nbsp;</td>\n",
    "<td colspan = 2>Predicted Positive</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan = 2>Actual Positive</td>\n",
    "<td>True Positives ($TP$)</td>\n",
    "<td>False Positives ($FP$)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>True Negatives ($TN$)</td>\n",
    "<td>False Negatives ($FN$)</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "We can define now define **precision** as the proportion of all documents predicted positive that actually are positive:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "And we can define **recall** as the proportion of all documents that actually are positive that are predicted positive:\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "If we can somehow balance the precision and recall, then we know we will have a good classifier that can accurately identify rare events. We balance these two metrics by taking the harmonic mean of them to get what is known as the **f1-score**:\n",
    "\n",
    "$$f_1 = 2 \\cdot \\frac{Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "Instead of accuracy, we will use the f1-score as our metric. We are finally ready to see how our classifier did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.882352941176\n",
      "Recall: 0.15306122449\n",
      "f1-score: 0.260869565217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "(precision, recall, f1, support) = precision_recall_fscore_support(y_test, y_predicted, average = 'binary')\n",
    "\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"f1-score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that of all the labels we predicted $1$, 88% of them were correctly predicted. However, we only correctly predicted label $1$ 15% of the time. This results in our overall f1-score being $0.26$. There is definitely room for improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Final Thoughts\n",
    "We ran a very basic logistic regression on our data, while there is so much more we can do. Here are some things we could consider trying next:\n",
    "* Improve our text processing. It is usually very helpful to throw out **stop words** such as \"the\", \"and\", and \"where\". It also helps to **stem** words, which maps different forms of a word to one form (i.e. \"go\" and \"went\" are both mapped to \"go\", or \"cat\" and \"cats\" are both mapped to \"cat\"). We can combine **named entities** into a single word, such as \"Microsoft Office\" or \"MacBook Pro\". The nltk library provides many functions to help with this kind of text processing.\n",
    "* Tune parameters of the model. If you look in sklearn's documentation for logistic regression you will see that you can pass many parameters into the LogisticRegression object, such as whether or not to include **regularization** or how to optimize the cost function.\n",
    "* Perform cross-validation. Instead of having one train and test set, you could partition the train set into $k$ **folds**, and train on $k-1$ of those and test on the remaining $1$. This can help elminate bias in parameter tuning.\n",
    "* Consider alternate binary classification algorithms. While logistgic regression works well, there are other common ones: Naive Bayes, support vector machine, decision tree, random forest. Sklearn offers support for all of these and they are just as easy to use as logistic regression (one call to train and one call to transform).\n",
    "\n",
    "If you would rather do multi-class classification, sklearn offers support for that as well $-$ for example, multinomial Naive Bayes or softmax regression. This would allow you to solve the problem of predicting the most likely of the 20 labels for a given document.\n",
    "\n",
    "Even though we only looked at an example in text processing, the techniques we have covered can be applied to anything. Consider image classification for example. If we design a matrix such that each row represents an image and each column represents the RGB value of a pixel, we could use the same algorithms and error metrics to classify pictures! As long as we can get a data matrix in this form, we can call these machine learning models on pretty much anything.\n",
    "\n",
    "Hope you enjoyed! Have fun at the hackathon!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
